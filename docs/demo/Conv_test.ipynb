{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80b8c0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- auto compile ---\n",
      "--- auto compile complete ---\n",
      "initiaze complete:\n",
      "  solver: None\n",
      " ---- hyperparameter ----\n",
      "  hyperparameter: tau_tilde=1.0\n",
      " ---- space ----\n",
      "  space: nx=1000, dx=0.0010, Lx=1.0\n",
      " ---- velocity ----\n",
      "  velocity: nv=200, dv=0.1005, v_max=10.0\n",
      " ---- time ----\n",
      "  time: nt=101, dt=0.0005, T_total=0.05\n",
      "  dtype: torch.float64\n",
      "  device: cuda, GPU name: NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "--- compile cuSOLVER ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file build/build.ninja...\n",
      "/home/arlm/workspace/venv/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module gtsv_batch...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] c++ -MMD -MF gtsv_binding.o.d -DTORCH_EXTENSION_NAME=gtsv_batch -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -I/usr/include/python3.12 -isystem /home/arlm/workspace/venv/lib/python3.12/site-packages/torch/include -isystem /home/arlm/workspace/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda-12.6/include -isystem /usr/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -O3 -c /home/arlm/workspace/kineticEQ/src/kineticEQ/backends/gtsv/gtsv_binding.cpp -o gtsv_binding.o \n",
      "[2/3] /usr/local/cuda-12.6/bin/nvcc --generate-dependencies-with-compile --dependency-output gtsv_batch.cuda.o.d -DTORCH_EXTENSION_NAME=gtsv_batch -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -I/usr/include/python3.12 -isystem /home/arlm/workspace/venv/lib/python3.12/site-packages/torch/include -isystem /home/arlm/workspace/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda-12.6/include -isystem /usr/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -std=c++17 -c /home/arlm/workspace/kineticEQ/src/kineticEQ/backends/gtsv/gtsv_batch.cu -o gtsv_batch.cuda.o \n",
      "[3/3] c++ gtsv_binding.o gtsv_batch.cuda.o -shared -lcusparse -L/home/arlm/workspace/venv/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda-12.6/lib64 -lcudart -o gtsv_batch.so\n",
      "--- compile CUDA fused implicit backend ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module gtsv_batch...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file build/build.ninja...\n",
      "Building extension module implicit_fused...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] /usr/local/cuda-12.6/bin/nvcc --generate-dependencies-with-compile --dependency-output implicit_kernels.cuda.o.d -DTORCH_EXTENSION_NAME=implicit_fused -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -I/usr/include/python3.12 -isystem /home/arlm/workspace/venv/lib/python3.12/site-packages/torch/include -isystem /home/arlm/workspace/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda-12.6/include -isystem /usr/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -std=c++17 -c /home/arlm/workspace/kineticEQ/src/kineticEQ/backends/implicit_fused/implicit_kernels.cu -o implicit_kernels.cuda.o \n",
      "[2/3] c++ -MMD -MF implicit_binding.o.d -DTORCH_EXTENSION_NAME=implicit_fused -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -I/usr/include/python3.12 -isystem /home/arlm/workspace/venv/lib/python3.12/site-packages/torch/include -isystem /home/arlm/workspace/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda-12.6/include -isystem /usr/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -O3 -c /home/arlm/workspace/kineticEQ/src/kineticEQ/backends/implicit_fused/implicit_binding.cpp -o implicit_binding.o \n",
      "[3/3] c++ implicit_binding.o implicit_kernels.cuda.o -shared -lcusparse -L/home/arlm/workspace/venv/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda-12.6/lib64 -lcudart -o implicit_fused.so\n",
      "--- fused CUDA backend loaded ---\n",
      "--- compile LO block-tridiag backend ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module implicit_fused...\n",
      "NoneType: None\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file build/build.ninja...\n",
      "Building extension module lo_blocktridiag...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] c++ -MMD -MF block_tridiag_binding.o.d -DTORCH_EXTENSION_NAME=lo_blocktridiag -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -I/usr/include/python3.12 -isystem /home/arlm/workspace/venv/lib/python3.12/site-packages/torch/include -isystem /home/arlm/workspace/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda-12.6/include -isystem /usr/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -O3 -c /home/arlm/workspace/kineticEQ/src/kineticEQ/backends/lo_blocktridiag/block_tridiag_binding.cpp -o block_tridiag_binding.o \n",
      "[2/3] /usr/local/cuda-12.6/bin/nvcc --generate-dependencies-with-compile --dependency-output block_tridiag_kernel.cuda.o.d -DTORCH_EXTENSION_NAME=lo_blocktridiag -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -I/usr/include/python3.12 -isystem /home/arlm/workspace/venv/lib/python3.12/site-packages/torch/include -isystem /home/arlm/workspace/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda-12.6/include -isystem /usr/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -std=c++17 -c /home/arlm/workspace/kineticEQ/src/kineticEQ/backends/lo_blocktridiag/block_tridiag_kernel.cu -o block_tridiag_kernel.cuda.o \n",
      "[3/3] c++ block_tridiag_binding.o block_tridiag_kernel.cuda.o -shared -L/home/arlm/workspace/venv/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda-12.6/lib64 -lcudart -o lo_blocktridiag.so\n",
      "--- LO block-tridiag backend loaded ---\n",
      "--- Convergence Test Start ---\n",
      "--- tau_tilde: 0.005 ---\n",
      "--- HOLO ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module lo_blocktridiag...\n",
      "NoneType: None\n",
      "Progress:   0%|                                                                 | 0/101 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/kineticEQ/tests/benchmarks/convergence_test.py:43\u001b[39m\n\u001b[32m     14\u001b[39m config = {\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mv_max\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m10.0\u001b[39m,\n\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdt\u001b[39m\u001b[33m\"\u001b[39m: args.dt,\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     40\u001b[39m }\n\u001b[32m     42\u001b[39m sim = BGK1DPlot(**config)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m conv_result = \u001b[43msim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_convergence_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m sim.save_benchmark_results(\n\u001b[32m     45\u001b[39m     filename=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs.output\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pkl\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     46\u001b[39m     bench_results=conv_result,\n\u001b[32m     47\u001b[39m )\n\u001b[32m     49\u001b[39m sim.plot_convergence_results(conv_result, filename=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs.output\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.png\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/kineticEQ/src/kineticEQ/BGK1Dsim.py:330\u001b[39m, in \u001b[36mBGK1D.run_convergence_test\u001b[39m\u001b[34m(self, tau_tilde_list)\u001b[39m\n\u001b[32m    327\u001b[39m     \u001b[38;5;28mself\u001b[39m.tau_tilde = tau\n\u001b[32m    329\u001b[39m     \u001b[38;5;66;03m# 収束性テストメソッド\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_holo_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m     \u001b[38;5;28mself\u001b[39m._run_picard_test()\n\u001b[32m    333\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- Convergence Test Completed ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/kineticEQ/src/kineticEQ/BGK1Dsim.py:353\u001b[39m, in \u001b[36mBGK1D._run_holo_test\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_progress_bar(\u001b[38;5;28mself\u001b[39m.use_tqdm,total=\u001b[38;5;28mself\u001b[39m.nt, desc=\u001b[33m\"\u001b[39m\u001b[33mProgress\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m    349\u001b[39m           bar_format=\u001b[33m'\u001b[39m\u001b[38;5;132;01m{l_bar}\u001b[39;00m\u001b[38;5;132;01m{bar}\u001b[39;00m\u001b[33m| \u001b[39m\u001b[38;5;132;01m{n_fmt}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{total_fmt}\u001b[39;00m\u001b[33m [\u001b[39m\u001b[38;5;132;01m{elapsed}\u001b[39;00m\u001b[33m<\u001b[39m\u001b[38;5;132;01m{remaining}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{rate_fmt}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m    351\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.nt):\n\u001b[32m    352\u001b[39m         \u001b[38;5;66;03m#更新実行\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m         ho_iter, ho_residual, lo_iter_list, lo_residual = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_implicit_update_holo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    355\u001b[39m         \u001b[38;5;66;03m# 進捗1%ごとに収集\u001b[39;00m\n\u001b[32m    356\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m step % progress_interval == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (step == \u001b[38;5;28mself\u001b[39m.nt - \u001b[32m1\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/kineticEQ/src/kineticEQ/BGK1Dsim.py:1267\u001b[39m, in \u001b[36mBGK1D._implicit_update_holo\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1264\u001b[39m lo_residual_list = []\n\u001b[32m   1266\u001b[39m \u001b[38;5;66;03m# HOLOアルゴリズム\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1267\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m z \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mho_iter\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1268\u001b[39m     \u001b[38;5;66;03m# z-1ステップのfk+1を用いて熱流束を計算\u001b[39;00m\n\u001b[32m   1269\u001b[39m     Q_HO = \u001b[38;5;28mself\u001b[39m._HO_calculate_fluxes(\u001b[38;5;28mself\u001b[39m._fz)\n\u001b[32m   1271\u001b[39m     \u001b[38;5;66;03m# LO_calculate_momentsによる次状態のモーメントの近似(線形化反復を含む)\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "%run kineticEQ/tests/benchmarks/convergence_test.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
